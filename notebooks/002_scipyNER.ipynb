{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        MISC       0.00      0.00      0.00         1\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.50      0.50      0.50         2\n",
      "   macro avg       0.50      0.50      0.50         2\n",
      "weighted avg       0.50      0.50      0.50         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG: {'TP': 1, 'TN': 0, 'FP': 0, 'FN': 1}\n",
      "PER: {'TP': 0, 'TN': 0, 'FP': 1, 'FN': 1}\n",
      "ORG: f1 0.6667, precision 1.0000, recall 0.5000\n",
      "PER: f1 0.0000, precision 0.0000, recall 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import List, Dict, Sequence\n",
    "\n",
    "class Matrics:\n",
    "    def __init__(self, sents_true_labels: Sequence[Sequence[Dict]], sents_pred_labels:Sequence[Sequence[Dict]]):\n",
    "        self.sents_true_labels = sents_true_labels\n",
    "        self.sents_pred_labels = sents_pred_labels \n",
    "        self.types = set(entity['type'] for sent in sents_true_labels for entity in sent)\n",
    "        self.confusion_matrices = {type: {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0} for type in self.types}\n",
    "        self.scores = {type: {'p': 0, 'r': 0, 'f1': 0} for type in self.types}\n",
    "\n",
    "    def cal_confusion_matrices(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Calculate confusion matrices for all sentences.\"\"\"\n",
    "        for true_labels, pred_labels in zip(self.sents_true_labels, self.sents_pred_labels):\n",
    "            for true_label in true_labels: \n",
    "                entity_type = true_label['type']\n",
    "                prediction_hit_count = 0 \n",
    "                for pred_label in pred_labels:\n",
    "                    if pred_label['type'] != entity_type:\n",
    "                        continue\n",
    "                    if pred_label['start_idx'] == true_label['start_idx'] and pred_label['end_idx'] == true_label['end_idx'] and pred_label['text'] == true_label['text']: # TP\n",
    "                        self.confusion_matrices[entity_type]['TP'] += 1\n",
    "                        prediction_hit_count += 1\n",
    "                    elif ((pred_label['start_idx'] == true_label['start_idx']) or (pred_label['end_idx'] == true_label['end_idx'])) and pred_label['text'] != true_label['text']: # boundry error, count FN, FP\n",
    "                        self.confusion_matrices[entity_type]['FP'] += 1\n",
    "                        self.confusion_matrices[entity_type]['FN'] += 1\n",
    "                        prediction_hit_count += 1\n",
    "                if prediction_hit_count != 1: # FN, model cannot make a prediction for true_label\n",
    "                    self.confusion_matrices[entity_type]['FN'] += 1\n",
    "                prediction_hit_count = 0 # reset to default\n",
    "\n",
    "    def cal_scores(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Calculate precision, recall, f1.\"\"\"\n",
    "        confusion_matrices = self.confusion_matrices \n",
    "        scores = {type: {'p': 0, 'r': 0, 'f1': 0} for type in self.types}\n",
    "        \n",
    "        for entity_type, confusion_matrix in confusion_matrices.items():\n",
    "            if confusion_matrix['TP'] == 0 and confusion_matrix['FP'] == 0:\n",
    "                scores[entity_type]['p'] = 0\n",
    "            else:\n",
    "                scores[entity_type]['p'] = confusion_matrix['TP'] / (confusion_matrix['TP'] + confusion_matrix['FP'])\n",
    "\n",
    "            if confusion_matrix['TP'] == 0 and confusion_matrix['FN'] == 0:\n",
    "                scores[entity_type]['r'] = 0\n",
    "            else:\n",
    "                scores[entity_type]['r'] = confusion_matrix['TP'] / (confusion_matrix['TP'] + confusion_matrix['FN']) \n",
    "\n",
    "            if scores[entity_type]['p'] == 0 or scores[entity_type]['r'] == 0:\n",
    "                scores[entity_type]['f1'] = 0\n",
    "            else:\n",
    "                scores[entity_type]['f1'] = 2*scores[entity_type]['p']*scores[entity_type]['r'] / (scores[entity_type]['p']+scores[entity_type]['r'])  \n",
    "        self.scores = scores\n",
    "\n",
    "    def print_confusion_matrices(self):\n",
    "        for entity_type, matrix in self.confusion_matrices.items():\n",
    "            print(f\"{entity_type}: {matrix}\")\n",
    "\n",
    "    def print_scores(self):\n",
    "        for entity_type, score in self.scores.items():\n",
    "            print(f\"{entity_type}: f1 {score['f1']:.4f}, precision {score['p']:.4f}, recall {score['r']:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    sents_true_labels = [[{'start_idx': 0, 'end_idx': 1, 'text': 'Foreign Ministry', 'type': 'ORG'}, \n",
    "                          {'start_idx': 3, 'end_idx': 4, 'text': 'Shen Guofang', 'type': 'PER'},\n",
    "                          {'start_idx': 6, 'end_idx': 6, 'text': 'Reuters', 'type': 'ORG'}]]\n",
    "                        \n",
    "    sents_pred_labels = [[{'start_idx': 3, 'end_idx': 3, 'text': 'Shen', 'type': 'PER'},\n",
    "                          {'start_idx': 6, 'end_idx': 6, 'text': 'Reuters', 'type': 'ORG'}]]\n",
    "\n",
    "    matrics = Matrics(sents_true_labels, sents_pred_labels)\n",
    "    matrics.cal_confusion_matrices()\n",
    "    matrics.print_confusion_matrices()\n",
    "    matrics.cal_scores()\n",
    "    matrics.print_scores()\n",
    "    \n",
    "# PER: {'TP': 0, 'TN': 0, 'FP': 1, 'FN': 1}\n",
    "# ORG: {'TP': 1, 'TN': 0, 'FP': 0, 'FN': 1}\n",
    "# PER: f1 0.0000, precision 0.0000, recall 0.0000\n",
    "# ORG: f1 0.6667, precision 1.0000, recall 0.5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "134d0c420be716f4705853afbda20cd302fe0206959a3d1297c95703aa0f5f9d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('c-m_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
